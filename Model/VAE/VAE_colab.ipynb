{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/pSzyc/Ising/main/Model/input_pipeline.py"
      ],
      "metadata": {
        "id": "auvQTeY1WjIx",
        "outputId": "62f8ad5c-226e-4ada-d7c2-1dcdf5ace416",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-22 05:42:27--  https://raw.githubusercontent.com/pSzyc/Ising/main/Model/input_pipeline.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2810 (2.7K) [text/plain]\n",
            "Saving to: ‘input_pipeline.py’\n",
            "\n",
            "\rinput_pipeline.py     0%[                    ]       0  --.-KB/s               \rinput_pipeline.py   100%[===================>]   2.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-22 05:42:27 (36.4 MB/s) - ‘input_pipeline.py’ saved [2810/2810]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7liWoc9QEgCT",
        "outputId": "65f5a60d-67b8-4582-8753-f520d3154e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import contextlib\n",
        "from PIL import Image\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "drive_path = Path(\"drive/MyDrive/Licencjat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cab88ObLEgCX"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Eh-BPPt5EgCa"
      },
      "outputs": [],
      "source": [
        "class VAE(tf.keras.Model):\n",
        "  \"\"\"Variational autoencoder.\"\"\"\n",
        "\n",
        "  def __init__(self, latent_dim, kernel_size = 3, lattice_size = 32):\n",
        "    super(VAE, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.encoder = tf.keras.Sequential(\n",
        "        [\n",
        "          tf.keras.layers.Conv2D(32, (kernel_size, kernel_size), activation='relu', padding=\"same\", input_shape=(32, 32, 1)),\n",
        "          tf.keras.layers.BatchNormalization(),\n",
        "          tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "          tf.keras.layers.Dropout(0.3),\n",
        "          tf.keras.layers.Conv2D(64, (kernel_size, kernel_size), padding=\"same\", activation='relu'),\n",
        "          tf.keras.layers.BatchNormalization(),\n",
        "          tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "          tf.keras.layers.Dropout(0.3),\n",
        "          tf.keras.layers.Conv2D(64, (kernel_size, kernel_size), padding=\"same\", activation='relu'),\n",
        "          tf.keras.layers.BatchNormalization(),\n",
        "          tf.keras.layers.Dropout(0.3),\n",
        "          tf.keras.layers.Flatten(),\n",
        "          tf.keras.layers.Dense(latent_dim + latent_dim)\n",
        "          ]\n",
        "     )\n",
        "\n",
        "    self.decoder = tf.keras.Sequential(\n",
        "        [\n",
        "          tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "          tf.keras.layers.Dense(units=8*8*32, activation=tf.nn.relu),\n",
        "          tf.keras.layers.Reshape(target_shape=(8, 8, 32)),\n",
        "          tf.keras.layers.Conv2DTranspose(filters=64,kernel_size=kernel_size, strides=(1, 1),padding=\"same\",activation='relu'),\n",
        "          tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
        "          tf.keras.layers.Conv2DTranspose(filters=64,kernel_size=kernel_size, strides=(1, 1),padding=\"same\",activation='relu'),\n",
        "          tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
        "          tf.keras.layers.Conv2DTranspose(filters=32,kernel_size=kernel_size, strides=(1, 1),padding=\"same\",activation='relu'),\n",
        "          tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=kernel_size, strides=(1, 1), padding=\"same\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  def call(self, inputs):\n",
        "    mean, logvar = self.encode(inputs)\n",
        "    z = self.reparameterize(mean, logvar)\n",
        "    reconstructed = self.decode(z)\n",
        "    return reconstructed\n",
        "\n",
        "  @tf.function\n",
        "  def sample(self, eps=None):\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "    return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "  def encode(self, x):\n",
        "    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "    return mean, logvar\n",
        "\n",
        "  def reparameterize(self, mean, logvar):\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "  def decode(self, z, apply_sigmoid=False):\n",
        "    logits = self.decoder(z)\n",
        "    if apply_sigmoid:\n",
        "      probs = tf.sigmoid(logits)\n",
        "      return probs\n",
        "    return logits\n",
        "\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "  log2pi = tf.math.log(2. * np.pi)\n",
        "  return tf.reduce_sum(\n",
        "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "      axis=raxis)\n",
        "\n",
        "\n",
        "def compute_loss(model, x):\n",
        "  mean, logvar = model.encode(x)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  x_logit = model.decode(z)\n",
        "  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "  logpz = log_normal_pdf(z, 0., 0.)\n",
        "  logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "  return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, x, optimizer):\n",
        "  \"\"\"Executes one training step and returns the loss.\n",
        "\n",
        "  This function computes the loss and gradients, and uses the latter to\n",
        "  update the model's parameters.\n",
        "  \"\"\"\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = compute_loss(model, x)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqqVHZudEgCe"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ab2VvreDEgCg"
      },
      "outputs": [],
      "source": [
        "def plot_spins(path, predictions, originals):\n",
        "    fig = plt.figure(figsize=(15, 20))\n",
        "    subfigs = fig.subfigures(nrows=len(predictions), ncols=1)\n",
        "\n",
        "    for index, subfig in enumerate(subfigs):\n",
        "        subfig.suptitle(f'Sample nr: {index}')\n",
        "        axs = subfig.subplots(nrows=1, ncols=4)\n",
        "\n",
        "        axs[0].imshow(predictions[index, :, :, 0], vmin=0, vmax=1)\n",
        "        axs[0].axis('off')\n",
        "        axs[0].set_title(\"Spin +1 probability map\")\n",
        "\n",
        "        axs[1].imshow(np.random.binomial(1, predictions[index, :, :, 0]), vmin=0, vmax=1)\n",
        "        axs[1].axis('off')\n",
        "        axs[1].set_title(\"Predicted from binomial\")\n",
        "\n",
        "        axs[2].imshow(predictions[index, :, :, 0] > 0.5, vmin=0, vmax=1)\n",
        "        axs[2].axis('off')\n",
        "        axs[2].set_title(\"p > 0.5\")\n",
        "\n",
        "        axs[3].imshow(originals[index, :, :, 0], vmin=0, vmax=1)\n",
        "        axs[3].axis('off')\n",
        "        axs[3].set_title(\"Original Sample\")\n",
        "\n",
        "    plt.savefig(path)\n",
        "    plt.show()\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_sample, results):\n",
        "  predictions = make_prediction(model, test_sample)\n",
        "  plot_spins(results / \"Images\" / f'image_at_epoch_{epoch}_{temp}.png', predictions, test_sample)\n",
        "\n",
        "def make_prediction(model, test_sample):\n",
        "    mean, logvar = model.encode(test_sample)\n",
        "    z = model.reparameterize(mean, logvar)\n",
        "    predictions = model.sample(z)\n",
        "    return predictions\n",
        "\n",
        "def sample(num_examples_to_generate, model):\n",
        "    random_mean = tf.random.normal(\n",
        "    shape=[num_examples_to_generate, latent_dim])\n",
        "    log_var = tf.random.normal(\n",
        "    shape=[num_examples_to_generate, latent_dim])\n",
        "\n",
        "    z = model.reparameterize(random_mean, log_var)\n",
        "    predictions = model.sample(z)\n",
        "    return predictions\n",
        "\n",
        "def make_gif(results, temp):\n",
        "  fp_in = results / f\"Images/*.png\"\n",
        "  fp_out = results /  f\"train_{temp}.gif\"\n",
        "\n",
        "  # use exit stack to automatically close opened images\n",
        "  with contextlib.ExitStack() as stack:\n",
        "\n",
        "      # lazily load images\n",
        "      imgs = (stack.enter_context(Image.open(f))\n",
        "              for f in sorted(glob.glob(str(fp_in))))\n",
        "\n",
        "      # extract  first image from iterator\n",
        "      img = next(imgs)\n",
        "\n",
        "      # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#gif\n",
        "      img.save(fp=str(fp_out), format='GIF', append_images=imgs,\n",
        "              save_all=True, duration=1000, loop=0)\n",
        "\n",
        "def calcEnergy(mat):\n",
        "    '''Energy of a given configuration'''\n",
        "    matrix_sum = get_neighbour_sum_matrix(mat)\n",
        "    return - np.sum(np.multiply(matrix_sum, mat))\n",
        "\n",
        "def calcMag(mat):\n",
        "    '''Magnetization of a given configuration'''\n",
        "    mag = np.sum(mat)\n",
        "    return mag\n",
        "\n",
        "def get_neighbour_sum_matrix(mat):\n",
        "    \"\"\"Matrix of the sum of spin values for all neighboring cells.\"\"\"\n",
        "    # Define shifts for different directions\n",
        "    shifts = [\n",
        "        (1, 1),  # Right\n",
        "        (-1, 1),  # Left\n",
        "        (-1, 0),  # Up\n",
        "        (1, 0),  # Down\n",
        "        (-1, 1),  # Up-Right\n",
        "        (1, 1),  # Down-Right\n",
        "        (-1, -1),  # Up-Left\n",
        "        (1, -1),  # Down-Left\n",
        "    ]\n",
        "\n",
        "    # Initialize an empty matrix for the sum of neighboring cells\n",
        "    neighbor_sum = np.zeros_like(mat)\n",
        "\n",
        "    # Iterate through each shift and accumulate the values in neighbor_sum\n",
        "    for shift in shifts:\n",
        "        shifted_mat = np.roll(mat, shift=shift, axis=(0, 1))\n",
        "        neighbor_sum += shifted_mat\n",
        "\n",
        "    return neighbor_sum\n",
        "\n",
        "def cost_function_plot(loss_list, results):\n",
        "  plt.figure()\n",
        "  plt.suptitle(\"Loss function vs Epoch - VAE\")\n",
        "  plt.plot(loss_list)\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Cost function\")\n",
        "  plt.savefig(results / \"loss_function.png\")\n",
        "\n",
        "def process_data(spin_image):\n",
        "  df = pd.DataFrame({'image': list(spin_image)})\n",
        "  df['Magnetization'] = df['image'].apply(calcMag)\n",
        "  df['Energy'] = df['image'].apply(calcEnergy)\n",
        "  df.drop(columns='image', inplace=True)\n",
        "  return df\n",
        "\n",
        "def sample_data_plots(model, test_size, batch_size, test_set, results):\n",
        "  sample_data = []\n",
        "  num = test_size\n",
        "  for _ in range(batch_size):\n",
        "    sample_data.append(sample(num, model))\n",
        "  spin_sample = tf.concat(sample_data, axis = 0)\n",
        "  preds = []\n",
        "  reals = []\n",
        "  for test_x in test_set:\n",
        "    preds.append(make_prediction(model, test_x))\n",
        "    reals.append(test_x)\n",
        "\n",
        "  predicted_spin = np.concatenate(preds)\n",
        "  real_spin = np.concatenate(reals)\n",
        "  predicted_spin =  2 * np.random.binomial(1, predicted_spin) - 1\n",
        "  real_spin =  2 * real_spin - 1\n",
        "  spin_sample = 2 * np.random.binomial(1, spin_sample) - 1\n",
        "  df = get_statistics_df(predicted_spin, real_spin, spin_sample)\n",
        "  sns.displot(df, x = 'Magnetization', kind ='kde', hue='Algorithm')\n",
        "  plt.savefig(results / f\"mag_{temp}.png\")\n",
        "  sns.displot(df, x = 'Energy', kind ='kde', hue='Algorithm')\n",
        "  plt.savefig(results / f\"energy_{temp}.png\")\n",
        "\n",
        "def get_statistics_df(perdicted_spin, real_spin, spin_sample):\n",
        "  df_model = process_data(perdicted_spin)\n",
        "  df_model['Algorithm'] = 'VAE testset'\n",
        "\n",
        "  df_test = process_data(real_spin)\n",
        "  df_test['Algorithm'] = 'Wolff testset'\n",
        "\n",
        "  df_sample = process_data(spin_sample)\n",
        "  df_sample ['Algorithm'] = 'VAE sample'\n",
        "\n",
        "  df = pd.concat([df_model, df_test, df_sample])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_starting_weights(model):\n",
        "    dummy_input = tf.zeros((1, 32, 32, 1))\n",
        "    model(dummy_input)\n",
        "    model.load_weights(drive_path / \"Results\" / \"reference\" / \"vae2.2.h5\")\n",
        "\n",
        "def load_weights_at_t(t, model):\n",
        "    dummy_input = tf.zeros((1, 32, 32, 1))\n",
        "    model(dummy_input)\n",
        "    model.load_weights(drive_path / \"Results\" / 'vae' / \"20-02\" / f\"{t:.2}\" / f\"vae{t:.2}.h5\")"
      ],
      "metadata": {
        "id": "YWWafAboAV3C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAx68FyGEgCU"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WOarJvM3EgCk"
      },
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "from time import time\n",
        "from queue import Queue\n",
        "from input_pipeline import dataset_tfrecord_pipeline\n",
        "\n",
        "def train_at_t(temp, model, batch_size = 100, epochs = 50, latent_dim = 200, num_examples_to_generate = 5, test_size = 100):\n",
        "  # Folders for storing the results and dataset\n",
        "  trainset_path = drive_path / \"data\" /  f\"Data{temp:.2}.tfrecord\"\n",
        "  testset_path = drive_path / \"data\" / 'test' / f\"TestData{temp:.2}.tfrecord\"\n",
        "\n",
        "  results = drive_path / \"Results\" / \"vae\" / \"22-02\" / f\"{temp:.2}\"\n",
        "  results.mkdir(exist_ok = True )\n",
        "\n",
        "  images = results / \"Images\"\n",
        "  images.mkdir(exist_ok = True)\n",
        "\n",
        "  train_set = dataset_tfrecord_pipeline(trainset_path, flatten=False, batch_size=batch_size)\n",
        "  test_set = dataset_tfrecord_pipeline(testset_path, flatten=False, batch_size=batch_size)\n",
        "  loss_monitor = 3 * [False]\n",
        "\n",
        "  # Test sample for image generation\n",
        "  assert batch_size >= num_examples_to_generate\n",
        "  for test_batch in test_set.take(1):\n",
        "    test_sample = test_batch[0:num_examples_to_generate, :, :, :]\n",
        "  generate_and_save_images(model, 0, test_sample, results)\n",
        "\n",
        "  # Training loop\n",
        "  loss_list = []\n",
        "  for epoch in range(1, epochs + 1):\n",
        "    time_start = time()\n",
        "    for train_x in train_set:\n",
        "      train_step(model, train_x, optimizer)\n",
        "    loss = tf.keras.metrics.Mean()\n",
        "    for test_x in test_set:\n",
        "      loss(compute_loss(model, test_x))\n",
        "\n",
        "    elbo = -loss.result()\n",
        "    loss_list.append(loss.result().numpy())\n",
        "\n",
        "    # Early stopping for VAE\n",
        "    if epoch > 1:\n",
        "      loss_monitor.pop(-1)\n",
        "      loss_monitor.insert(0, loss_list[-1] - loss_list[-2] > 0)\n",
        "      if all(loss_monitor):\n",
        "        print(\"STOP!\")\n",
        "        break\n",
        "\n",
        "    # Spins from testset\n",
        "    display.clear_output(wait=False)\n",
        "    generate_and_save_images(model, epoch, test_sample, results)\n",
        "    elapsed = time() - time_start\n",
        "    print(f'Epoch: {epoch}, Test set ELBO: {elbo}, Time needed: {elapsed}')\n",
        "\n",
        "  # Results\n",
        "  make_gif(results, temp)\n",
        "  cost_function_plot(loss_list, results)\n",
        "  sample_data_plots(model, test_size, batch_size, test_set, results)\n",
        "  model.save_weights(results / f'vae{temp:.2}.h5',save_format='h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "latent_dim = 200\n",
        "num_examples_to_generate = 5\n",
        "batch_size = 100\n",
        "temps = [t for t in np.arange(2.0, 3.1, 0.1)]\n",
        "model = VAE(latent_dim, lattice_size = 32)\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "zHeWTKaqpFoH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, temp in enumerate(temps):\n",
        "  load_weights_at_t(temp, model)\n",
        "  train_at_t(temp, model, batch_size, epochs, latent_dim, num_examples_to_generate)"
      ],
      "metadata": {
        "id": "bjP2Ttckp43c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXPl__CcV_fT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
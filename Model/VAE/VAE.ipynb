{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 18:31:55.106527: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-27 18:31:55.149318: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 18:31:55.149394: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 18:31:55.150476: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 18:31:55.156072: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-27 18:31:55.156692: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-27 18:31:56.265880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from input_pipeline import dataset_pipeline, get_param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your TFRecord file\n",
    "tfrecord_file = \"../../GetData/Python/Data/trainset.tfrecord\"\n",
    "\n",
    "# Create a TFRecordDataset\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
    "\n",
    "# Define the feature description\n",
    "feature_description = {\n",
    "    'data': tf.io.FixedLenFeature([32 * 32], tf.int64),  # Adjust the shape and dtype accordingly\n",
    "}\n",
    "\n",
    "# Parsing function to parse the example\n",
    "def _parse_function(example_proto):\n",
    "    parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    data_feature = parsed_example['data']\n",
    "    return  data_feature\n",
    "\n",
    "# Apply the parsing function to each element in the dataset\n",
    "dataset = dataset.map(_parse_function)\n",
    "dataset = dataset.map(lambda x: tf.reshape(x, [32, 32]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.batch(64)\n",
    "dataset = dataset.map(lambda x: tf.cast(x, tf.float32), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Now you can iterate over the dataset and access the parsed features\n",
    "for batch in dataset.take(1):\n",
    "    print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Now you can iterate over the dataset and access the parsed features\n",
    "for batch in dataset.take(1):\n",
    "    for example in batch:\n",
    "        print(example.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_path = \"../GetData/Rust/get_data_rust/Data/TrainsetW\"\n",
    "testset_path = \"../GetData/Rust/get_data_rust/Data/TestsetW\"\n",
    "batch_size = 100\n",
    "train_set = dataset_pipeline(trainset_path, flatten=False, batch_size=batch_size)\n",
    "train_params = get_param_dict(trainset_path)\n",
    "test_set = dataset_pipeline(testset_path, flatten=False, batch_size=batch_size)\n",
    "test_params = get_param_dict(testset_path)\n",
    "\n",
    "print(\"Trainset parameters: \" + str(train_params))\n",
    "print(\"Testset parameters: \" + str(test_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "  \"\"\"Variational autoencoder.\"\"\"\n",
    "\n",
    "  def __init__(self, latent_dim, kernel_size = 3):\n",
    "    super(VAE, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.encoder = tf.keras.Sequential(\n",
    "        [\n",
    "          tf.keras.layers.Conv2D(32, kernel_size, activation='relu', padding=\"same\", input_shape=(64, 64, 1)),\n",
    "          tf.keras.layers.BatchNormalization(),\n",
    "          tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "          tf.keras.layers.Dropout(0.5),\n",
    "          tf.keras.layers.Conv2D(64, kernel_size, padding=\"same\", activation='relu'),\n",
    "          tf.keras.layers.BatchNormalization(),\n",
    "          tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "          tf.keras.layers.Dropout(0.5),\n",
    "          tf.keras.layers.Conv2D(64, kernel_size, padding=\"same\", activation='relu'),\n",
    "          tf.keras.layers.BatchNormalization(),\n",
    "          tf.keras.layers.Dropout(0.5),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          tf.keras.layers.Dense(latent_dim + latent_dim)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    self.decoder = tf.keras.Sequential(\n",
    "        [\n",
    "          tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "          tf.keras.layers.Dense(units=16*16*32, activation=tf.nn.relu),\n",
    "          tf.keras.layers.Reshape(target_shape=(16, 16, 32)),\n",
    "          tf.keras.layers.Conv2DTranspose(64, kernel_size, padding=\"same\",activation='relu'),\n",
    "          tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "          tf.keras.layers.Conv2DTranspose(64, kernel_size, padding=\"same\",activation='relu'),\n",
    "          tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "          tf.keras.layers.Conv2DTranspose(32, kernel_size, padding=\"same\",activation='relu'),\n",
    "          tf.keras.layers.Conv2DTranspose(1, kernel_size,  padding=\"same\"),\n",
    "        ]\n",
    "    )\n",
    "  \n",
    "  @tf.function\n",
    "  def sample(self, eps=None):\n",
    "    if eps is None:\n",
    "      eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "    return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "  def encode(self, x):\n",
    "    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "    return mean, logvar\n",
    "\n",
    "  def reparameterize(self, mean, logvar):\n",
    "    eps = tf.random.normal(shape=mean.shape)\n",
    "    return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "  def decode(self, z, apply_sigmoid=False):\n",
    "    logits = self.decoder(z)\n",
    "    if apply_sigmoid:\n",
    "      probs = tf.sigmoid(logits)\n",
    "      return probs\n",
    "    return logits\n",
    "  \n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "  log2pi = tf.math.log(2. * np.pi)\n",
    "  return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis)\n",
    "\n",
    "\n",
    "def compute_loss(model, x):\n",
    "  mean, logvar = model.encode(x)\n",
    "  z = model.reparameterize(mean, logvar)\n",
    "  x_logit = model.decode(z)\n",
    "  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "  logpz = log_normal_pdf(z, 0., 0.)\n",
    "  logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "  return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "  \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "  This function computes the loss and gradients, and uses the latter to\n",
    "  update the model's parameters.\n",
    "  \"\"\"\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = compute_loss(model, x)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "epochs = 150\n",
    "latent_dim = 100\n",
    "\n",
    "num_examples_to_generate = 5\n",
    "random_vector_for_generation = tf.random.normal(\n",
    "    shape=[num_examples_to_generate, latent_dim])\n",
    "\n",
    "model = VAE(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spins(path, predictions, originals):\n",
    "    fig = plt.figure(figsize=(15, 20))\n",
    "    subfigs = fig.subfigures(nrows=len(predictions), ncols=1)\n",
    "\n",
    "    for index, subfig in enumerate(subfigs):\n",
    "        subfig.suptitle(f'Sample nr: {index}')\n",
    "        axs = subfig.subplots(nrows=1, ncols=4)\n",
    "        \n",
    "        axs[0].imshow(predictions[index, :, :, 0], vmin=0, vmax=1)\n",
    "        axs[0].axis('off')\n",
    "        axs[0].set_title(\"Spin +1 probability map\")\n",
    "\n",
    "        axs[1].imshow(np.random.binomial(1, predictions), vmin=0, vmax=1)\n",
    "        axs[1].axis('off')\n",
    "        axs[1].set_title(\"Predicted spins\")\n",
    "\n",
    "        axs[2].imshow(originals[index, :, :, 0], vmin=0, vmax=1)\n",
    "        axs[2].axis('off')\n",
    "        axs[2].set_title(\"Original Sample\")\n",
    "        \n",
    "        axs[3].imshow(np.abs(originals[index, :, :, 0] - np.round(predictions[index, :, :, 0])), cmap = 'Reds', vmin=0, vmax=1)\n",
    "        axs[3].axis('off')\n",
    "        axs[3].set_title(\"Difference map\")\n",
    "\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_sample):\n",
    "  predictions = make_prediction(model, test_sample)\n",
    "  plot_spins('train_img/image_at_epoch_{:04d}.png'.format(epoch), predictions, test_sample)\n",
    "\n",
    "def make_prediction(model, test_sample):\n",
    "    mean, logvar = model.encode(test_sample)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    predictions = model.sample(z)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample of the test set for generating output images\n",
    "assert batch_size >= num_examples_to_generate\n",
    "for test_batch in test_set.take(1):\n",
    "  test_sample = test_batch[0:num_examples_to_generate, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from time import time\n",
    "loss_list = []\n",
    "generate_and_save_images(model, 0, test_sample)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "  time_start = time()\n",
    "  for train_x in train_set:\n",
    "    train_step(model, train_x, optimizer)\n",
    "  loss = tf.keras.metrics.Mean()\n",
    "  for test_x in test_set:\n",
    "    loss(compute_loss(model, test_x))\n",
    "  elbo = -loss.result()\n",
    "  loss_list.append(loss.result())\n",
    "  display.clear_output(wait=False)\n",
    "  if epoch%5==0:\n",
    "    generate_and_save_images(model, epoch, test_sample)\n",
    "  elapsed = time() - time_start\n",
    "  print(f'Epoch: {epoch}, Test set ELBO: {elbo}, Time needed: {elapsed}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle(\"Loss function vs Epoch - VAE\")\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"Cost function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import contextlib\n",
    "from PIL import Image\n",
    "\n",
    "# filepaths\n",
    "fp_in = \"train_img/*.png\"\n",
    "fp_out = \"train_slow.gif\"\n",
    "\n",
    "# use exit stack to automatically close opened images\n",
    "with contextlib.ExitStack() as stack:\n",
    "\n",
    "    # lazily load images\n",
    "    imgs = (stack.enter_context(Image.open(f))\n",
    "            for f in sorted(glob.glob(fp_in)))\n",
    "\n",
    "    # extract  first image from iterator\n",
    "    img = next(imgs)\n",
    "\n",
    "    # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#gif\n",
    "    img.save(fp=fp_out, format='GIF', append_images=imgs,\n",
    "             save_all=True, duration=2000, loop=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/vae.h5',save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(num_examples_to_generate):\n",
    "    random_mean = tf.random.normal(\n",
    "    shape=[num_examples_to_generate, latent_dim])\n",
    "    log_var = tf.random.normal(\n",
    "    shape=[num_examples_to_generate, latent_dim])\n",
    "\n",
    "    z = model.reparameterize(random_mean, log_var)\n",
    "    predictions = model.sample(z)\n",
    "    return tf.squeeze(predictions)\n",
    "\n",
    "num_examples_to_generate = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sample(num_examples_to_generate)\n",
    "plt.suptitle(\"20 image probabilities generated by a model\")\n",
    "for i in range(num_examples_to_generate):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.gca().axis('off')\n",
    "    plt.imshow(predictions[i, : , :], vmin=0, vmax=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating 1000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_examples_to_generate = 1000\n",
    "predictions = sample(num_examples_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spin_predicted = 2 * np.random.binomial(1, predictions) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare distributions with Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sys.path.insert(0, \"../GetData/\")\n",
    "from macro import calcEnergy, calcMag, get_distribution_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.DataFrame({'image': list(spin_predicted)})\n",
    "df_model['Magnetization'] = df_model['image'].apply(calcMag)\n",
    "df_model['Energy'] = df_model['image'].apply(calcEnergy)\n",
    "df_model.drop(columns='image', inplace=True)\n",
    "df_model['Algorithm'] = 'cVAE'\n",
    "\n",
    "df_test = get_distribution_data('../GetData/Rust/get_data_rust/Data/TestsetW')\n",
    "df_test.drop(columns = ['folder', 'Steps'], inplace=True)\n",
    "\n",
    "df = pd.concat([df_test, df_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df, x = 'Magnetization', kind ='kde', hue='Algorithm')\n",
    "sns.displot(df, x = 'Energy', kind ='kde', hue='Algorithm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

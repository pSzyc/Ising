{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ising Model GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "from Plots.style import *\n",
    "from Model.input_pipeline import dataset_tfrecord_pipeline\n",
    "import time\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "data_path = Path(\"../../GetData/Python/Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_plots(generator, batch_size, outdir):\n",
    "  sample_data = []\n",
    "  reals = []\n",
    "  for image, temp in test_dataset:\n",
    "    image = plus_encode_data(image)\n",
    "    random_vector_for_generation = tf.random.normal([batch_size, 1, 1, noise_dim])\n",
    "    generated = generator(random_vector_for_generation)\n",
    "    sample_data.append(np.random.binomial(1, generated))\n",
    "    reals.append(image)\n",
    "  temp = temp.numpy()[0]\n",
    "\n",
    "  generated_spins = np.concatenate(sample_data)\n",
    "  real_spins = np.concatenate(reals)\n",
    "  generated_spins =  2 * (generated_spins > 0) - 1\n",
    "  real_spins =  2 * real_spins - 1\n",
    "  generated_spins = random_swap(generated_spins)\n",
    "  real_spins = random_swap(real_spins)\n",
    "\n",
    "  df_test = process_data(real_spins)\n",
    "  df_test['Algorithm'] = 'Wolff testset'\n",
    "\n",
    "  df_sample = process_data(generated_spins)\n",
    "  df_sample['Algorithm'] = 'GAN Generator'\n",
    "\n",
    "  assert len(df_sample) == len(df_test)\n",
    "  df = pd.concat([df_test, df_sample])\n",
    "    \n",
    "  sns.displot(df, x = 'Magnetization', kind ='kde', hue='Algorithm')\n",
    "  plt.savefig(outdir / f\"mag_{temp}.svg\")\n",
    "  sns.displot(df, x = 'Energy', kind ='kde', hue='Algorithm')\n",
    "  plt.savefig(outdir / f\"energy_{temp}.svg\")\n",
    "\n",
    "def process_data(spin_image):\n",
    "  df = pd.DataFrame({'image': list(spin_image)})\n",
    "  df['Magnetization'] = df['image'].apply(calcMag)\n",
    "  df['Energy'] = df['image'].apply(calcEnergy)\n",
    "  df.drop(columns='image', inplace=True)\n",
    "  return df\n",
    "    \n",
    "def random_swap(data):\n",
    "    data = tf.reshape(data, shape = (data.shape[0], -1))\n",
    "    swap_mask = 2 * (np.random.random(size = (data.shape[0], 1)) > 0.5) - 1\n",
    "    data = tf.multiply(data, swap_mask)\n",
    "    data = tf.reshape(data, shape = (-1, 32, 32, 1))\n",
    "    return data\n",
    "    \n",
    "def calcEnergy(mat):\n",
    "    '''Energy of a given configuration'''\n",
    "    matrix_sum = get_neighbour_sum_matrix(mat)\n",
    "    return - np.sum(np.multiply(matrix_sum, mat))\n",
    "\n",
    "def calcMag(mat):\n",
    "    '''Magnetization of a given configuration'''\n",
    "    mag = np.sum(mat)\n",
    "    return mag\n",
    "    \n",
    "def get_neighbour_sum_matrix(mat):\n",
    "    \"\"\"Matrix of the sum of spin values for all neighboring cells.\"\"\"\n",
    "    # Define shifts for different directions\n",
    "    shifts = [\n",
    "        (1, 1),  # Right\n",
    "        (-1, 1),  # Left\n",
    "        (-1, 0),  # Up\n",
    "        (1, 0),  # Down\n",
    "        (-1, 1),  # Up-Right\n",
    "        (1, 1),  # Down-Right\n",
    "        (-1, -1),  # Up-Left\n",
    "        (1, -1),  # Down-Left\n",
    "    ]\n",
    "\n",
    "    # Initialize an empty matrix for the sum of neighboring cells\n",
    "    neighbor_sum = np.zeros_like(mat)\n",
    "\n",
    "    # Iterate through each shift and accumulate the values in neighbor_sum\n",
    "    for shift in shifts:\n",
    "        shifted_mat = np.roll(mat, shift=shift, axis=(0, 1))\n",
    "        neighbor_sum += shifted_mat\n",
    "\n",
    "    return neighbor_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input, outdir):\n",
    "    predictions = tf.round(model(test_input, training=False))\n",
    "    \n",
    "    plt.figure(figsize=(4,4))\n",
    "    \n",
    "    for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0])\n",
    "      plt.axis('off')\n",
    "        \n",
    "    plt.savefig(outdir / 'image_at_epoch_{:04d}.svg'.format(epoch))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_cost_function(outdir):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(np.asarray(disc_loss_log), color='tab:red')\n",
    "    ax1.set_xlabel('Batch')\n",
    "    ax1.set_ylabel('Discriminator Loss', color='tab:red')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(np.asarray(gen_loss_log), color='tab:blue')\n",
    "    ax2.set_ylabel('Generator Loss', color='tab:blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    plt.savefig(outdir / 'cost_function.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIterator:\n",
    "    def __init__(self, datasets, temps):\n",
    "        self.datasets = datasets\n",
    "        self.temps = temps\n",
    "    def __iter__(self):\n",
    "        self.data_iterators = [iter(data) for data in self.datasets]\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        data_list = []\n",
    "        temp_list = []\n",
    "        for index, data_iterator in enumerate(self.data_iterators):\n",
    "            data = next(data_iterator)\n",
    "            data_list.append(data)\n",
    "            temp = self.temps[index] * np.ones((data.shape[0], 1))\n",
    "            temp_list.append(temp)\n",
    "        temps = tf.concat(temp_list, axis=0)\n",
    "        data = tf.concat(data_list, axis=0)\n",
    "        return data, temps\n",
    "        \n",
    "def make_dataset(data_dir, temps, batch_size=100, flatten=False):\n",
    "    if isinstance(data_dir, str):\n",
    "        data_dir = Path(data_dir)\n",
    "    \n",
    "    assert batch_size % len(temps) == 0, \"Batch size must be divisible by the number of temperatures\"\n",
    "\n",
    "    trainset = []\n",
    "    testset = []\n",
    "    for temp in temps:\n",
    "        dataset = dataset_tfrecord_pipeline(data_dir / f\"Data{temp:.1f}.tfrecord\", flatten=flatten, batch_size=batch_size // len(temps))\n",
    "        trainset.append(dataset)\n",
    "        dataset = dataset_tfrecord_pipeline(data_dir / f\"TestData{temp:.1f}.tfrecord\", flatten=flatten, batch_size=batch_size // len(temps))\n",
    "        testset.append(dataset)\n",
    "    gen_test = DataIterator(testset, temps)\n",
    "    test_dataset = tf.data.Dataset.from_generator(lambda: gen_test, output_signature = (tf.TensorSpec(shape=(None, 32, 32, 1), dtype=tf.float32), tf.TensorSpec(shape=(None, 1), dtype=tf.float32)))\n",
    "    gen_train = DataIterator(trainset, temps)\n",
    "    train_dataset = tf.data.Dataset.from_generator(lambda: gen_train, output_signature = (tf.TensorSpec(shape=(None, 32, 32, 1), dtype=tf.float32), tf.TensorSpec(shape=(None, 1), dtype=tf.float32)))\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def plus_encode_data(data):\n",
    "    data = tf.reshape(data, shape=(data.shape[0], -1))\n",
    "    mean_data = tf.reduce_mean(data, axis=1, keepdims=True)\n",
    "    need_change = tf.cast(mean_data < 0.5, data.dtype)\n",
    "    data = data * (1 - 2 * need_change) + need_change\n",
    "    data = tf.reshape(data, shape=(-1, 32, 32, 1))\n",
    "    return data   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(nc = 1, ngf = 64):\n",
    "    model = tf.keras.Sequential([\n",
    "        # input is Z, going into a convolution\n",
    "        tf.keras.layers.Conv2DTranspose(ngf * 8, (4, 4), strides=(1, 1), padding='valid', use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        # state size. (ngf*8) x 4 x 4\n",
    "        tf.keras.layers.Conv2DTranspose(ngf * 4, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        # state size. (ngf*4) x 8 x 8\n",
    "        tf.keras.layers.Conv2DTranspose(ngf * 2, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        # state size. (ngf*2) x 16 x 16\n",
    "        tf.keras.layers.Conv2DTranspose(nc, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
    "        tf.keras.layers.Activation('sigmoid')\n",
    "        # state size. (nc) x 32 x 32\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model(ndf = 64):\n",
    "    model = tf.keras.Sequential([\n",
    "        # input is (nc) x 32 x 32\n",
    "        tf.keras.layers.Conv2D(ndf, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # state size. (ndf) x 16 x 16\n",
    "        tf.keras.layers.Conv2D(ndf * 2, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # state size. (ndf*2) x 8 x 8\n",
    "        tf.keras.layers.Conv2D(ndf * 4, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # state size. (ndf*4) x 4 x 4\n",
    "        tf.keras.layers.Conv2D(1, (4, 4), strides=(1, 1), padding='valid', use_bias=False),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.Conv2DTranspose):\n",
    "            layer.weights[0] = tf.keras.initializers.RandomNormal(mean=0., stddev=0.02)(shape=layer.weights[0].shape)\n",
    "        elif isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.weights[0] = tf.keras.initializers.RandomNormal(mean=1., stddev=0.02)(shape=layer.weights[0].shape)\n",
    "            layer.weights[1] = tf.keras.initializers.Zeros()(shape=layer.weights[1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch, gen_loss_log, disc_loss_log, batch_size, noise_dim, generator, discriminator, generator_optimizer, discriminator_optimizer, regression_model):\n",
    "      noise = tf.random.normal([batch_size, 1, 1, noise_dim])\n",
    "      images, temps = batch\n",
    "      images = plus_encode_data(images)\n",
    "      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generator's images\n",
    "        generated_images = generator(noise, training=True)\n",
    "        \n",
    "          \n",
    "        # Estimated temperature by regression network\n",
    "        estimated_temps = regression_model(generated_images)\n",
    "\n",
    "        # Discriminator part  \n",
    "        real_logits = discriminator(images, training=True)\n",
    "        generated_logits = discriminator(generated_images, training=True)\n",
    "\n",
    "        # Cost Function\n",
    "        gen_loss = generator_loss(generated_logits, estimated_temps, temps)\n",
    "        gen_loss_log.append(gen_loss)\n",
    "        disc_loss = discriminator_loss(real_logits, generated_logits)\n",
    "        disc_loss_log.append(disc_loss)\n",
    "        \n",
    "      gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "      \n",
    "      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "def train(dataset, epochs, batch_size, gen_loss_log, disc_loss_log, regression_model, outdir):  \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for batch in tqdm(dataset): \n",
    "          train_step(\n",
    "                 batch, \n",
    "                 gen_loss_log, \n",
    "                 disc_loss_log, \n",
    "                 batch_size, \n",
    "                 noise_dim, \n",
    "                 generator, \n",
    "                 discriminator, \n",
    "                 generator_optimizer, \n",
    "                 discriminator_optimizer,\n",
    "                 regression_model\n",
    "          )\n",
    "        print (f\"Time taken for epoch {epoch} is {time.time()- start} sec\")\n",
    "    display.clear_output(wait=True) \n",
    "    generate_and_save_images(\n",
    "      generator,\n",
    "      epoch + 1,\n",
    "      random_vector_for_generation,\n",
    "      outdir\n",
    "    )\n",
    "\n",
    "def generator_loss(generated_logits, estimated_temps, temps, lambda2 = 1e3):\n",
    "    l1 = tf.losses.binary_crossentropy(tf.ones_like(generated_logits), generated_logits, from_logits = True)\n",
    "    # Regression network\n",
    "    l2 = tf.reduce_mean(tf.losses.mean_squared_error(estimated_temps, temps))\n",
    "    return tf.reduce_mean(l1) + lambda2 * l2\n",
    "\n",
    "def discriminator_loss(real_image, generated_image):\n",
    "    real_loss = tf.losses.binary_crossentropy(tf.ones_like(real_image), real_image, from_logits = True)\n",
    "    generated_loss = tf.losses.binary_crossentropy(tf.zeros_like(generated_image), generated_image, from_logits = True)\n",
    "    total_loss = tf.reduce_mean(real_loss) + tf.reduce_mean(generated_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "random_vector_for_generation = tf.random.normal([num_examples_to_generate, 1, 1, noise_dim])\n",
    "beta1 = 0.5\n",
    "lr = 1e-4\n",
    "EPOCHS=10\n",
    "temps = np.arange(2.0, 2.11, 0.1)\n",
    "batch_size = 240\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Format it to include date, hour, and minutes\n",
    "formatted_datetime = current_datetime.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "results = Path(\"results/\" + formatted_datetime)\n",
    "results.mkdir()\n",
    "\n",
    "regression_model = tf.keras.models.load_model(\"results/regression_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in [2.2]:\n",
    "    outdir = results / f\"{temp:.1f}\"\n",
    "    outdir.mkdir(exist_ok = True)\n",
    "    gen_loss_log=[]\n",
    "    disc_loss_log=[]\n",
    "    generator = make_generator_model()\n",
    "    discriminator = make_discriminator_model()\n",
    "    \n",
    "    generator_optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=beta1, beta_2=0.999)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=beta1, beta_2=0.999)\n",
    "    \n",
    "    discriminator.build(input_shape = (None, 32, 32, 1))\n",
    "    generator.build(input_shape = (None, 1, 1, 100))\n",
    "    \n",
    "    initialize_weights(generator)\n",
    "    initialize_weights(discriminator)\n",
    "    \n",
    "    train_dataset, test_dataset = make_dataset(data_path, [temp], batch_size=batch_size, flatten=False)\n",
    "    train(train_dataset, EPOCHS, batch_size, gen_loss_log, disc_loss_log, regression_model, outdir)\n",
    "    \n",
    "    plot_cost_function(outdir)\n",
    "    sample_data_plots(generator, batch_size, outdir)\n",
    "    discriminator.save(outdir / \"discriminator\")\n",
    "    generator.save(outdir / \"generator\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ising Model GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "import time\n",
    "from IPython import display\n",
    "from model import RegressionModel\n",
    "from pathlib import Path\n",
    "data_path = Path(\"../../GetData/Python/Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def train_step(batch, gen_loss_log, disc_loss_log, batch_size, noise_dim, generator, discriminator, generator_optimizer, discriminator_optimizer, regression_model):\n",
    "      noise = tf.random.normal([batch_size, noise_dim])\n",
    "      images, temps = batch\n",
    "      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        estimated_temps = regression_model(generated_images)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        generated_output = discriminator(generated_images, training=True)\n",
    "        gen_loss = generator_loss(generated_output, estimated_temps, temps)\n",
    "        gen_loss_log.append(gen_loss)\n",
    "        disc_loss = discriminator_loss(real_output, generated_output)\n",
    "        disc_loss_log.append(disc_loss)\n",
    "        \n",
    "      gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "      \n",
    "      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "def generator_loss(generated_image, estimated_temps, temps, lambda2 = 100):\n",
    "    # todo L2, L3\n",
    "    l1 = tf.reduce_mean(tf.losses.binary_crossentropy(tf.ones_like(generated_image), generated_image))\n",
    "    l2 = tf.reduce_mean(tf.losses.mean_squared_error(estimated_temps, temps))\n",
    "    return l1 + lambda2 * l2\n",
    "\n",
    "def discriminator_loss(real_image, generated_image):\n",
    "    real_loss = tf.losses.binary_crossentropy(tf.ones_like(real_image), real_image, from_logits = True)\n",
    "    generated_loss = tf.losses.binary_crossentropy(tf.zeros_like(generated_image), generated_image, from_logits = True)\n",
    "    total_loss = real_loss + generated_loss\n",
    "    return tf.reduce_mean(total_loss)\n",
    "\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(4 * 4 * 1024, use_bias=False, input_shape=(100,)))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Reshape((4, 4, 1024)))\n",
    "    assert model.output_shape == (None, 4, 4, 1024)\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(512, (1, 1), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None,  8, 8, 512)  \n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 16, 16, 256)  \n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 32, 32, 128)    \n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation=tf.sigmoid))\n",
    "\n",
    "    assert model.output_shape == (None, 32, 32, 1)\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "      \n",
    "    model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "       \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "     \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from input_pipeline import dataset_tfrecord_pipeline\n",
    "\n",
    "class DataIterator:\n",
    "    def __init__(self, datasets, temps):\n",
    "        self.datasets = datasets\n",
    "        self.temps = temps\n",
    "    def __iter__(self):\n",
    "        self.data_iterators = [iter(data) for data in self.datasets]\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        data_list = []\n",
    "        temp_list = []\n",
    "        for index, data_iterator in enumerate(self.data_iterators):\n",
    "            data = next(data_iterator)\n",
    "            data_list.append(data)\n",
    "            temp = self.temps[index] * np.ones((data.shape[0], 1))\n",
    "            temp_list.append(temp)\n",
    "        temps = tf.concat(temp_list, axis=0)\n",
    "        data = tf.concat(data_list, axis=0)\n",
    "        return data, temps\n",
    "        \n",
    "def make_dataset(data_dir, temps, batch_size=100, flatten=False):\n",
    "    if isinstance(data_dir, str):\n",
    "        data_dir = Path(data_dir)\n",
    "    \n",
    "    assert batch_size % len(temps) == 0, \"Batch size must be divisible by the number of temperatures\"\n",
    "\n",
    "    trainset = []\n",
    "    testset = []\n",
    "    for temp in temps:\n",
    "        dataset = dataset_tfrecord_pipeline(data_dir / f\"Data{temp:.1f}.tfrecord\", flatten=flatten, batch_size=batch_size // len(temps))\n",
    "        trainset.append(dataset)\n",
    "        dataset = dataset_tfrecord_pipeline(data_dir / f\"TestData{temp:.1f}.tfrecord\", flatten=flatten, batch_size=batch_size // len(temps))\n",
    "        testset.append(dataset)\n",
    "    gen_test = DataIterator(testset, temps)\n",
    "    test_dataset = tf.data.Dataset.from_generator(lambda: gen_test, output_signature = (tf.TensorSpec(shape=(None, 32, 32, 1), dtype=tf.float32), tf.TensorSpec(shape=(None, 1), dtype=tf.float32)))\n",
    "    gen_train = DataIterator(trainset, temps)\n",
    "    train_dataset = tf.data.Dataset.from_generator(lambda: gen_train, output_signature = (tf.TensorSpec(shape=(None, 32, 32, 1), dtype=tf.float32), tf.TensorSpec(shape=(None, 1), dtype=tf.float32)))\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = np.arange(2.0, 3.1, 0.1)\n",
    "train_dataset, _ = make_dataset(data_path, temps, batch_size=110, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Format it to include date, hour, and minutes\n",
    "formatted_datetime = current_datetime.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "results = Path(\"results/\" + formatted_datetime)\n",
    "results.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  predictions = tf.round(model(test_input, training=False))\n",
    "\n",
    "  plt.figure(figsize=(4,4))\n",
    "  \n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0])\n",
    "      plt.axis('off')\n",
    "        \n",
    "  plt.savefig(results / 'image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()\n",
    "\n",
    "def train(dataset, epochs, batch_size, gen_loss_log, disc_loss_log, regression_model):  \n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for batch in tqdm(dataset): \n",
    "      train_step(\n",
    "             batch, \n",
    "             gen_loss_log, \n",
    "             disc_loss_log, \n",
    "             batch_size, \n",
    "             noise_dim, \n",
    "             generator, \n",
    "             discriminator, \n",
    "             generator_optimizer, \n",
    "             discriminator_optimizer,\n",
    "             regression_model)\n",
    "\n",
    "    display.clear_output(wait=True) \n",
    "    generate_and_save_images(\n",
    "      generator,\n",
    "      epoch + 1,\n",
    "      random_vector_for_generation\n",
    "    ) \n",
    "    print (f\"Time taken for epoch {epoch} is {time.time()- start} sec\")\n",
    "    \n",
    "\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "random_vector_for_generation = tf.random.normal([num_examples_to_generate,\n",
    "                                                 noise_dim])\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "regression_model = tf.keras.models.load_model(\"results/regression_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, temp in train_dataset.take(1):\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    print(\"Temp shape:\", temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=1\n",
    "gen_loss_log=[]\n",
    "disc_loss_log=[]\n",
    "batch_size=110 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS, batch_size, gen_loss_log, disc_loss_log, regression_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss of the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(np.asarray(disc_loss_log), color='tab:red')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Discriminator Loss', color='tab:red')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.asarray(gen_loss_log), color='tab:blue')\n",
    "ax2.set_ylabel('Generator Loss', color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_weights('generator_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "predictions = generator(random_vector_for_generation, training=False)\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    spins = np.random.binomial(1, predictions[i, :, :])\n",
    "    plt.imshow(spins)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

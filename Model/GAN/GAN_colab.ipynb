{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJIIRCwhEYGj"
      },
      "source": [
        "## Ising Model GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trSZesBRLFA5",
        "outputId": "5096aa45-feee-4905-af68-db1f54b4ed61"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/pSzyc/Ising/main/Model/GAN/model.py\n",
        "!wget https://raw.githubusercontent.com/pSzyc/Ising/main/Model/input_pipeline.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULt_ebFhEYGl",
        "outputId": "a91e620e-90bd-4bdf-ee1d-aa10ed8f64a7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import time\n",
        "from IPython import display\n",
        "from input_pipeline import dataset_tfrecord_pipeline\n",
        "from model import make_discriminator_model, make_generator_model, generator_loss, discriminator_loss\n",
        "import csv\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "data_path = \"drive/MyDrive/Licencjat\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN2Jr9L7EYGm"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV5ZO7laEYGn",
        "outputId": "811028b0-7340-4823-fc83-a12d702bc5f1"
      },
      "outputs": [],
      "source": [
        "trainset_path = f\"{data_path}/trainset.tfrecord\"\n",
        "testset_path =  f\"{data_path}/testset.tfrecord\"\n",
        "batch_size = 120\n",
        "train_set = dataset_tfrecord_pipeline(trainset_path, flatten=False, batch_size=batch_size)\n",
        "test_set = dataset_tfrecord_pipeline(testset_path, flatten=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E--E6qcuEYGn"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXCWdZwREYGn"
      },
      "outputs": [],
      "source": [
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq11JovvEYGn"
      },
      "outputs": [],
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4AzdPLDEYGn"
      },
      "source": [
        "## Setup training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-pRYEuuEYGo"
      },
      "outputs": [],
      "source": [
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# We'll re-use this random vector used to seed the generator so\n",
        "# it will be easier to see the improvement over time.\n",
        "random_vector_for_generation = tf.random.normal([num_examples_to_generate,\n",
        "                                                 noise_dim])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVtGI4idEYGo"
      },
      "outputs": [],
      "source": [
        "def train_step(images, gen_loss_log, disc_loss_log):\n",
        "      noise = tf.random.normal([batch_size, noise_dim])\n",
        "\n",
        "      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        generated_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(generated_output)\n",
        "        gen_loss_log.append(gen_loss)\n",
        "        disc_loss = discriminator_loss(real_output, generated_output)\n",
        "        disc_loss_log.append(disc_loss)\n",
        "\n",
        "      gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7ybuluhEYGo"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  predictions = tf.round(model(test_input, training=False))\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(np.rint(predictions[i, :, :, 0]))\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4ulPvxkEYGo"
      },
      "outputs": [],
      "source": [
        "def train(dataset, epochs, gen_loss_log, disc_loss_log):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for images in tqdm(dataset): train_step(images, gen_loss_log, disc_loss_log)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(\n",
        "      generator,\n",
        "      epoch + 1,\n",
        "      random_vector_for_generation\n",
        "    )\n",
        "    print (f\"Time taken for epoch {epoch} is {time.time()- start} sec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzLVkB79EYGp"
      },
      "source": [
        "## Train the GAN!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9upxctybEYGp"
      },
      "outputs": [],
      "source": [
        "EPOCHS=30\n",
        "gen_loss_log=[]\n",
        "disc_loss_log=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "O1vDb-pqEYGp",
        "outputId": "41508188-a5f4-4e26-b579-b4df18e22f10"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train(train_set, EPOCHS, gen_loss_log, disc_loss_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeOtnaSTEYGp"
      },
      "source": [
        "## Plot the loss of the generator and discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ngg7pbY-EYGq",
        "outputId": "c041b672-344f-4a2f-b829-d3d5d6796633",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.plot(np.asarray(gen_loss_log))\n",
        "plt.plot(np.asarray(disc_loss_log))\n",
        "plt.legend(['Generator', 'Discriminator'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abnLiPeZEYGq"
      },
      "outputs": [],
      "source": [
        "predictions = generator(random_vector_for_generation, training=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "ce5CTI08U9iE",
        "outputId": "9b9ae092-a206-4c5c-e894-608a820f845b"
      },
      "outputs": [],
      "source": [
        "for data in test_set.take(1):\n",
        "  sns.heatmap(data[0, :, :, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBRgrS7zEYGq"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(predictions[0, :, :, 0]>0)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

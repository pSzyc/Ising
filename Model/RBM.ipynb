{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 07:34:25.364878: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-16 07:34:25.411496: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-16 07:34:26.509441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from input_pipeline import get_param_dict, benchmark\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy_file(file_path):\n",
    "    return np.load(file_path)\n",
    "\n",
    "def load_numpy_file_wrapper(file_path):\n",
    "    return tf.numpy_function(load_npy_file, [file_path], tf.int32)\n",
    "\n",
    "def dataset_pipeline_old(path, flatten=True, batch_size=1):\n",
    "    print(\"Getting data from \" + path)\n",
    "    dataset = tf.data.Dataset.list_files(f\"{path}/*/*.npy\")\n",
    "    print(f\"Got {len(dataset)} samples\")\n",
    "    dataset = dataset.map(load_numpy_file_wrapper)\n",
    "    if flatten:\n",
    "        dataset = dataset.map(lambda x: tf.reshape(x, [-1]))\n",
    "    else:\n",
    "        dataset = dataset.map(lambda x: tf.expand_dims(x, 2))\n",
    "    dataset = dataset.map(lambda x: (x+1)/2)\n",
    "    dataset = dataset.map(lambda x: tf.cast(x, tf.float32))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "def dataset_pipeline(path, flatten=True, batch_size=1):\n",
    "    print(\"Getting data from \" + path)\n",
    "    dataset = tf.data.Dataset.list_files(f\"{path}/*/*.npy\")\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    print(f\"Got {len(dataset)} samples\")\n",
    "    dataset = dataset.map(load_numpy_file_wrapper)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if flatten:\n",
    "        dataset = dataset.map(lambda x: tf.reshape(x, [batch_size, -1]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        dataset = dataset.map(lambda x: tf.expand_dims(x, 3), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x: (x+1)/2, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x: tf.cast(x, tf.float32), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../GetData/Rust/get_data_rust/Data/Trainset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters used in this dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Steps': '1000',\n",
       " 'Simulatiton Number': '10000',\n",
       " 'Temperature': '2.73',\n",
       " 'Magnetic Field': '0',\n",
       " 'Mattize Size': '64'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = get_param_dict(path)\n",
    "print(\"Parameters used in this dataset:\")\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from ../GetData/Rust/get_data_rust/Data/Trainset\n",
      "Got 10000 samples\n",
      "Number of examples:  10000\n",
      "Execution time: 3.892809787000033\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_pipeline(path, batch_size=100, flatten=False)\n",
    "benchmark(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricted Boltzman Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    def __init__(self, num_visible, num_hidden, learning_rate):\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W = tf.Variable(tf.random.normal([num_visible, num_hidden]))\n",
    "        self.visible_bias = tf.Variable(tf.zeros([num_visible]))\n",
    "        self.hidden_bias = tf.Variable(tf.zeros([num_hidden]))\n",
    "\n",
    "    def sample_h_given_v(self, v):\n",
    "        p_h_given_v = tf.nn.sigmoid(tf.matmul(v, self.W) + self.hidden_bias)\n",
    "        p = tf.random.uniform(shape = p_h_given_v.shape)\n",
    "        h_new = tf.cast(p_h_given_v <= p, dtype=tf.float32)\n",
    "        return h_new, p_h_given_v\n",
    "\n",
    "    def sample_v_given_h(self, h):\n",
    "        p_v_given_h = tf.nn.sigmoid(tf.matmul(h, tf.transpose(self.W)) + self.visible_bias)\n",
    "        p = tf.random.uniform(shape = p_v_given_h.shape)\n",
    "        v_new = tf.cast(p_v_given_h <= p, dtype=tf.float32)\n",
    "        return v_new\n",
    "    \n",
    "    def train(self, dataset, num_epochs):\n",
    "        n = len(dataset)\n",
    "        for epoch in range(num_epochs):\n",
    "            reconstruction_loss = 0\n",
    "            for v0 in tqdm(dataset):\n",
    "                h0, h0_prob = self.sample_h_given_v(v0)\n",
    "                v_new = self.sample_v_given_h(h0)\n",
    "                h_new, h_new_prob = self.sample_h_given_v(v0)\n",
    "                reconstruction_loss += tf.reduce_mean(tf.square(v0 - v_new))\n",
    "                self.backward(v0, h0_prob, v_new, h_new_prob)\n",
    "            print(f\"Epoch {epoch}/{num_epochs}, Average Loss: {reconstruction_loss/n:.5f}\")\n",
    "        print(\"Training completed.\")\n",
    "\n",
    "    def backward(self, v0, h0_prob, v_new, h_new_prob):\n",
    "        d_vb = tf.reduce_sum(v0, axis=0)-tf.reduce_sum(v_new, axis=0)\n",
    "        d_hb = tf.reduce_sum(h0_prob, axis=0)-tf.reduce_sum(h_new_prob, axis=0)\n",
    "        d_W = tf.matmul(tf.transpose(v0), h0_prob) - tf.matmul(tf.transpose(v_new), h_new_prob)\n",
    "        self.W.assign_add(self.learning_rate * d_W)\n",
    "        self.hidden_bias.assign_add(self.learning_rate * d_hb)\n",
    "        self.visible_bias.assign_add(self.learning_rate * d_vb)\n",
    "\n",
    "    def generate(self, num_samples):\n",
    "        samples = tf.random.uniform(shape=[num_samples, self.num_visible])\n",
    "        for _ in range(100):  # Perform 100 Gibbs sampling steps for mixing\n",
    "            hidden_samples, _ = self.sample_h_given_v(samples)\n",
    "            samples = self.sample_v_given_h(hidden_samples)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm = RBM(num_visible=int(params['Mattize Size'])**2, num_hidden=64, learning_rate=0.0001)\n",
    "rbm.train(dataset, num_epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare model to monte carlo algorithm\n",
    "The goal of this project is to recreate probability distribution of spin configurations in Ising Model using deep learning methods <br>\n",
    "Below few samples from monte carlo dataset are compared with those created by RBM model. They dont have to be the same as we are sampling randomly but they should<br>\n",
    "be similar on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generated = rbm.generate(10)\n",
    "data_sample = list(dataset.take(1))[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plot(data_sample, data_generated):\n",
    "    n_rows = len(data_sample)\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(8, 32))\n",
    "    subfigs = fig.subfigures(nrows=n_rows, ncols=1)\n",
    "\n",
    "    for index, subfig in enumerate(subfigs):\n",
    "        axs = subfig.subplots(nrows=1, ncols=2)\n",
    "\n",
    "        axs[0].imshow(tf.reshape(data_generated[index], (64,64)) )\n",
    "        axs[0].set_title(f'RBM')\n",
    "\n",
    "        axs[1].imshow(tf.reshape(data_sample[index], (64,64)) )\n",
    "        axs[1].set_title(f'Monte Carlo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot(data_sample, data_generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
